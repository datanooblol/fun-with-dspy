{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4b24612",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ff8a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "if hasattr(dspy, 'cache') and hasattr(dspy.cache, 'disk_cache'):\n",
    "    dspy.cache.disk_cache.clear()\n",
    "    print(\"clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce31ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSignature(dspy.Signature):\n",
    "    \"\"\"Respond to user in a conversation\"\"\"\n",
    "    conversation_history: str = dspy.InputField(desc=\"Previous messages\")\n",
    "    user_message: str = dspy.InputField(desc=\"Current user input\")\n",
    "    response: str = dspy.OutputField(desc=\"Assistant's response\")\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.predictor = dspy.Predict(ChatSignature)\n",
    "        self.history = []\n",
    "    \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        # Format history\n",
    "        history_str = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.history])\n",
    "        \n",
    "        # Get response using signature\n",
    "        result = self.predictor(conversation_history=history_str, user_message=user_message)\n",
    "        \n",
    "        # Update history\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": result.response})\n",
    "        \n",
    "        return result.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74f2934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.base import DriverLM, ModelResponse, Usage\n",
    "import httpx\n",
    "from typing import Any\n",
    "\n",
    "ollama_client = httpx.Client(timeout=600.0)\n",
    "\n",
    "def ollama_request_fn(messages:list[dict[str, Any]], temperature:float=0.0, **kwargs)->dict:\n",
    "    base_url = 'http://localhost:11434/api/chat'\n",
    "\n",
    "    response = ollama_client.post(\n",
    "        base_url,\n",
    "        json={\n",
    "            \"model\": \"llama3.2-vision:11b\",\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()  # Return full Ollama response\n",
    "\n",
    "def ollama_output_fn(response:dict)->ModelResponse:\n",
    "    content = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "    model = response.get(\"model\", \"custom\")\n",
    "    \n",
    "    usage = Usage(\n",
    "        prompt_tokens=response.get(\"prompt_eval_count\", 0),\n",
    "        completion_tokens=response.get(\"eval_count\", 0),\n",
    "        total_tokens=response.get(\"prompt_eval_count\", 0) + response.get(\"eval_count\", 0)\n",
    "    )\n",
    "    \n",
    "    return ModelResponse.from_text(text=content.strip(), usage=usage, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbaac45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-1e974956-1cfe-45e5-9aae-7ece836e85e1', created=1768728070, model='llama3.2-vision:11b', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='How can I assist you?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 10, 'completion_tokens': 7, 'total_tokens': 17}, cache_hit=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama_request_fn(messages=[{\"role\": \"user\", \"content\": \"Hi\"}])\n",
    "ollama_output_fn(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50794d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_lm = DriverLM(\n",
    "    request_fn=ollama_request_fn,\n",
    "    output_fn=ollama_output_fn,\n",
    "    cache=True\n",
    ")\n",
    "\n",
    "ollama_lm.clear_cache()  # Clear old cache entries\n",
    "\n",
    "dspy.configure(lm=ollama_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d42438ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSignature(dspy.Signature):\n",
    "    \"\"\"Response to user in a conversation\"\"\"\n",
    "    chat_history:str = dspy.InputField(desc=\"Previous chat history\")\n",
    "    user_message:str = dspy.InputField(desc=\"Current user message\")\n",
    "    response:str = dspy.OutputField(desc=\"Assistant's resposne\")\n",
    "\n",
    "class ChatAgent(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.program = dspy.Predict(ChatSignature)\n",
    "        self.history = []\n",
    "\n",
    "    def forward(self, message:str):\n",
    "        if len(self.history)>0:\n",
    "            chat_history = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in self.history])\n",
    "        else:\n",
    "            chat_history = \"No previous messages\"\n",
    "        response = self.program(chat_history=chat_history, user_message=message)\n",
    "        self.history.append({\"role\": \"user\", \"content\": message})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": response.response})\n",
    "        return response.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a63a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.configure(lm=ollama_lm)\n",
    "ca = ChatAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1202a70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/18 16:32:59 WARNING dspy.primitives.module: Calling module.forward(...) on ChatAgent directly is discouraged. Please use module(...) instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you too, Bank! What brings you here today?'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca.forward(message=\"My name is Bank. Nice to meet you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9aff9831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bank.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(message=\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34af1e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input fields are:\n",
      "1. `chat_history` (str): Previous chat history\n",
      "2. `user_message` (str): Current user message\n",
      "Your output fields are:\n",
      "1. `response` (str): Assistant's resposne\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chat_history ## ]]\n",
      "{chat_history}\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "{user_message}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Response to user in a conversation\n"
     ]
    }
   ],
   "source": [
    "print(ollama_lm.history[-1]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc7c06b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ ## chat_history ## ]]\n",
      "No previous messages\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "My name is Bank. Nice to meet you.\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n"
     ]
    }
   ],
   "source": [
    "print(ollama_lm.history[-1]['messages'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99ba3742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': None,\n",
       " 'messages': [{'role': 'system',\n",
       "   'content': \"Your input fields are:\\n1. `chat_history` (str): Previous chat history\\n2. `user_message` (str): Current user message\\nYour output fields are:\\n1. `response` (str): Assistant's resposne\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## chat_history ## ]]\\n{chat_history}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Response to user in a conversation\"},\n",
       "  {'role': 'user',\n",
       "   'content': '[[ ## chat_history ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nMy name is Bank. Nice to meet you.\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}],\n",
       " 'kwargs': {},\n",
       " 'response': ModelResponse(id='chatcmpl-ed42d1e8-52a0-4873-aca6-c3042f779dc5', created=1768728609, model='llama3.2-vision:11b', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## response ## ]]\\nNice to meet you too, Bank! What brings you here today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={}, cache_hit=True),\n",
       " 'outputs': ['[[ ## response ## ]]\\nNice to meet you too, Bank! What brings you here today?'],\n",
       " 'usage': {},\n",
       " 'cost': 0.0,\n",
       " 'timestamp': '2026-01-18T16:32:59.892736',\n",
       " 'uuid': '3c5b9815-deff-4241-a16f-6835c27c6cb2',\n",
       " 'model': 'llama3.2-vision:11b',\n",
       " 'response_model': 'llama3.2-vision:11b',\n",
       " 'model_type': 'chat'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama_lm.history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8d699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fun-with-dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
