{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4b24612",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ff8a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "\n",
    "if hasattr(dspy, 'cache') and hasattr(dspy.cache, 'disk_cache'):\n",
    "    dspy.cache.disk_cache.clear()\n",
    "    print(\"clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce31ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSignature(dspy.Signature):\n",
    "    \"\"\"Respond to user in a conversation\"\"\"\n",
    "    conversation_history: str = dspy.InputField(desc=\"Previous messages\")\n",
    "    user_message: str = dspy.InputField(desc=\"Current user input\")\n",
    "    response: str = dspy.OutputField(desc=\"Assistant's response\")\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.predictor = dspy.Predict(ChatSignature)\n",
    "        self.history = []\n",
    "    \n",
    "    def chat(self, user_message: str) -> str:\n",
    "        # Format history\n",
    "        history_str = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in self.history])\n",
    "        \n",
    "        # Get response using signature\n",
    "        result = self.predictor(conversation_history=history_str, user_message=user_message)\n",
    "        \n",
    "        # Update history\n",
    "        self.history.append({\"role\": \"user\", \"content\": user_message})\n",
    "        self.history.append({\"role\": \"assistant\", \"content\": result.response})\n",
    "        \n",
    "        return result.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f2934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from package.base import DriverLM, ModelResponse, Usage\n",
    "import httpx\n",
    "from typing import Any\n",
    "\n",
    "ollama_client = httpx.Client(timeout=600.0)\n",
    "\n",
    "def ollama_request_fn(messages:list[dict[str, Any]], temperature:float=0.0, **kwargs)->dict:\n",
    "    base_url = 'http://localhost:11434/api/chat'\n",
    "    # print(messages)\n",
    "    response = ollama_client.post(\n",
    "        base_url,\n",
    "        json={\n",
    "            \"model\": \"llama3.2-vision:11b\",\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature}\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    return response.json()  # Return full Ollama response\n",
    "\n",
    "def ollama_output_fn(response:dict)->ModelResponse:\n",
    "    content = response.get(\"message\", {}).get(\"content\", \"\")\n",
    "    model = response.get(\"model\", \"custom\")\n",
    "    \n",
    "    usage = Usage(\n",
    "        prompt_tokens=response.get(\"prompt_eval_count\", 0),\n",
    "        completion_tokens=response.get(\"eval_count\", 0),\n",
    "        total_tokens=response.get(\"prompt_eval_count\", 0) + response.get(\"eval_count\", 0)\n",
    "    )\n",
    "    \n",
    "    return ModelResponse.from_text(text=content.strip(), usage=usage, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbaac45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-77d3b091-9730-4d3e-9abb-b2e6af72a927', created=1768742690, model='llama3.2-vision:11b', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='How can I assist you?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 10, 'completion_tokens': 7, 'total_tokens': 17}, cache_hit=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama_request_fn(messages=[{\"role\": \"user\", \"content\": \"Hi\"}])\n",
    "ollama_output_fn(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e5a140e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import Any\n",
    "from package.base import DriverLM, ModelResponse, Usage\n",
    "\n",
    "\n",
    "def bedrock_request_fn(messages: list[dict[str, Any]], temperature: float = 0.0, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Request function for AWS Bedrock Converse API.\n",
    "    \n",
    "    DSPy passes messages in OpenAI format:\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"...\"},\n",
    "        {\"role\": \"user\", \"content\": \"...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    Bedrock Converse API expects:\n",
    "    - system: list of system messages (separate parameter)\n",
    "    - messages: list of user/assistant turns (no system role)\n",
    "    \"\"\"\n",
    "    \n",
    "    client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "    \n",
    "    # Separate system messages from conversation\n",
    "    system_messages = []\n",
    "    conversation_messages = []\n",
    "    \n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            system_messages.append({\"text\": msg[\"content\"]})\n",
    "        else:\n",
    "            conversation_messages.append({\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": [{\"text\": msg[\"content\"]}]\n",
    "            })\n",
    "    \n",
    "    # Build request\n",
    "    request_params = {\n",
    "        \"modelId\": \"us.amazon.nova-lite-v1:0\",\n",
    "        \"messages\": conversation_messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": kwargs.get(\"max_tokens\", 2048),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add system messages if present\n",
    "    if system_messages:\n",
    "        request_params[\"system\"] = system_messages\n",
    "    \n",
    "    # Call Bedrock\n",
    "    response = client.converse(**request_params)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def bedrock_output_fn(response: dict) -> ModelResponse:\n",
    "    \"\"\"\n",
    "    Parse Bedrock Converse API response into ModelResponse.\n",
    "    \n",
    "    Bedrock response format:\n",
    "    {\n",
    "        \"output\": {\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"text\": \"...\"}]\n",
    "            }\n",
    "        },\n",
    "        \"usage\": {\n",
    "            \"inputTokens\": 100,\n",
    "            \"outputTokens\": 50,\n",
    "            \"totalTokens\": 150\n",
    "        },\n",
    "        \"stopReason\": \"end_turn\",\n",
    "        \"metrics\": {...}\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract content\n",
    "    content = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    \n",
    "    # Extract usage\n",
    "    usage_data = response.get(\"usage\", {})\n",
    "    usage = Usage(\n",
    "        prompt_tokens=usage_data.get(\"inputTokens\", 0),\n",
    "        completion_tokens=usage_data.get(\"outputTokens\", 0),\n",
    "        total_tokens=usage_data.get(\"totalTokens\", 0)\n",
    "    )\n",
    "    \n",
    "    # Get model ID from response metadata\n",
    "    model = response.get(\"ResponseMetadata\", {}).get(\"HTTPHeaders\", {}).get(\"x-amzn-bedrock-model-id\", \"bedrock-model\")\n",
    "    \n",
    "    return ModelResponse.from_text(text=content, usage=usage, model=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3030d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-1aab7ca6-af66-4798-9b6d-c076ddef45f6', created=1768742695, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! How can I assist you today? If you have any questions or need information on a particular topic, feel free to ask. Whether it's about science, technology, history, or something else, I'm here to help.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 1, 'completion_tokens': 50, 'total_tokens': 51}, cache_hit=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = bedrock_request_fn(messages=[{\"role\": \"user\", \"content\": \"Hi\"}])\n",
    "bedrock_output_fn(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50794d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_lm = DriverLM(\n",
    "    request_fn=ollama_request_fn,\n",
    "    output_fn=ollama_output_fn,\n",
    "    cache=True\n",
    ")\n",
    "\n",
    "ollama_lm.clear_cache()  # Clear old cache entries\n",
    "\n",
    "native_lm = dspy.LM(\n",
    "    model=\"ollama/llama3.2-vision:11b\",\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Create Bedrock LM\n",
    "bedrock_lm = DriverLM(\n",
    "    request_fn=bedrock_request_fn,\n",
    "    output_fn=bedrock_output_fn,\n",
    "    temperature=0.0,\n",
    "    max_tokens=2048,\n",
    "    cache=True\n",
    ")\n",
    "\n",
    "bedrock_lm.clear_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d42438ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSignature(dspy.Signature):\n",
    "    \"\"\"Use the conversation_context to answer questions about the user.\"\"\"\n",
    "    conversation_context: str = dspy.InputField(desc=\"All previous messages in this conversation\")\n",
    "    user_message: str = dspy.InputField(desc=\"Current question\")\n",
    "    response: str = dspy.OutputField(desc=\"Answer using conversation_context\")\n",
    "\n",
    "\n",
    "demos = [\n",
    "    dspy.Example(\n",
    "        conversation_context=\"No previous messages\",\n",
    "        user_message=\"Hi, my name is Alice\",\n",
    "        response=\"Hello Alice! Nice to meet you.\"\n",
    "    ).with_inputs(\"conversation_context\", \"user_message\"),\n",
    "    \n",
    "    dspy.Example(\n",
    "        conversation_context=\"USER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you.\",\n",
    "        user_message=\"What's my name?\",\n",
    "        response=\"Your name is Alice.\"  # No greeting\n",
    "    ).with_inputs(\"conversation_context\", \"user_message\"),\n",
    "    \n",
    "    # Add this demo:\n",
    "    dspy.Example(\n",
    "        conversation_context=\"USER: Hi, my name is Alice\\nASSISTANT: Hello Alice!\\nUSER: What's my name?\\nASSISTANT: Your name is Alice.\",\n",
    "        user_message=\"How are you?\",\n",
    "        response=\"I'm doing well, thank you for asking!\"  # No greeting\n",
    "    ).with_inputs(\"conversation_context\", \"user_message\"),\n",
    "]\n",
    "\n",
    "# Use demos in ChatAgent\n",
    "class ChatAgent(dspy.Module):\n",
    "    def __init__(self, demos=None):\n",
    "        self.program = dspy.Predict(ChatSignature)\n",
    "        if demos:\n",
    "            self.program.demos = demos\n",
    "        self.chat_history = []\n",
    "\n",
    "    def forward(self, message:str):\n",
    "        if len(self.chat_history) > 0:\n",
    "            chat_history = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in self.chat_history])\n",
    "        else:\n",
    "            chat_history = \"No previous messages\"\n",
    "        response = self.program(conversation_context=chat_history, user_message=message)\n",
    "        self.chat_history.append({\"role\": \"user\", \"content\": message})\n",
    "        self.chat_history.append({\"role\": \"assistant\", \"content\": response.response})\n",
    "        return response.response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a63a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = ollama_lm\n",
    "# lm = native_lm\n",
    "lm = bedrock_lm\n",
    "dspy.configure(lm=lm)\n",
    "# ca = ChatAgent()\n",
    "# Create agent with demos\n",
    "ca = ChatAgent(demos=demos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1202a70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nice to meet you too, Bank!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(message=\"My name is Bank. Nice to meet you.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cc4762a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have information about your favorite pizza topping. I don't have access to personal preferences unless you've shared them with me in this conversation.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hallucination from demos\n",
    "ca(message=\"What's my favorite pizza topping?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbe8bea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great to hear! Hawaiian pizza is a popular choice. If you ever want to talk about pizza or anything else, feel free to share!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(message=\"For your previous answer, I love hawaiian!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e84e8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There are so many delicious pizza toppings to try! Some popular ones include pepperoni, mushrooms, onions, bell peppers, olives, and bacon. If you're feeling adventurous, you might want to try something unique like pineapple, jalapeños, or even pineapple and ham. The best way to find your new favorite topping is to experiment and see what you enjoy the most. Don't be afraid to try something new!\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(message=\"If I wanna find my new favorite pizza topping, what would you recommend?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aff9831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Bank.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca(message=\"What's my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ca77e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'My name is Bank. Nice to meet you.'},\n",
       " {'role': 'assistant', 'content': \"Hello Bank! It's nice to meet you.\"},\n",
       " {'role': 'user', 'content': \"What's my favorite pizza topping?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm sorry, but I don't have that information. It's best to ask you directly about your favorite pizza topping.\"},\n",
       " {'role': 'user', 'content': 'For your previous answer, I love hawaiian!'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"It's great to hear that you love Hawaiian pizza! It's a delicious choice.\"},\n",
       " {'role': 'user',\n",
       "  'content': 'If I wanna find my new favorite pizza topping, what would you recommend?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"That's a fun question! If you're looking to try something new, I'd recommend trying a pizza with truffle oil and mushrooms. The earthy flavor of the mushrooms combined with the richness of truffle oil can create a unique and delicious experience. However, the best topping is always personal preference, so feel free to experiment and find what you love!\"},\n",
       " {'role': 'user', 'content': \"What's my name?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm sorry, but I don't have your name stored in our conversation. However, earlier you mentioned your name is Bank.\"}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5b10f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ ## chat_history ## ]]\n",
      "USER: My name is Bank. Nice to meet you.\n",
      "ASSISTANT: Hello Bank! It's nice to meet you.\n",
      "USER: What's my favorite pizza topping?\n",
      "ASSISTANT: I don't have that information, but if you'd like to share, I'd be happy to hear about your favorite pizza topping!\n",
      "USER: For your previous answer, I love hawaiian!\n",
      "ASSISTANT: That sounds delicious! Hawaiian pizza with pineapple and ham is a great choice!\n",
      "USER: If I wanna find my new favorite pizza topping, what would you recommend?\n",
      "ASSISTANT: That's a fun question! If you're looking to try something new and delicious, I'd recommend trying a pizza with figs and prosciutto. It's a unique combination that pairs wonderfully with the creamy cheese and crispy crust. Another great option is a pizza with spicy chorizo and peppers – it's bold and flavorful!\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "What's my name?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n"
     ]
    }
   ],
   "source": [
    "print(lm.history[-1]['messages'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34af1e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input fields are:\n",
      "1. `chat_history` (str): Actual conversation history\n",
      "2. `user_message` (str): Current user message\n",
      "Your output fields are:\n",
      "1. `response` (str): Assistant's response\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chat_history ## ]]\n",
      "{chat_history}\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "{user_message}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Respond based only on the provided chat_history. Examples are for format only.\n"
     ]
    }
   ],
   "source": [
    "print(lm.history[-1]['messages'][0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc7c06b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ ## chat_history ## ]]\n",
      "USER: What's my favorite pizza topping?\n",
      "ASSISTANT: I'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\n",
      "USER: My name is Bank. Nice to meet you.\n",
      "ASSISTANT: Hello Bank! It's nice to meet you. If you ever want to share your favorite pizza topping, I'm here to listen.\n",
      "USER: What's my name?\n",
      "ASSISTANT: Hello again! Your name is Bank. It's nice to meet you.\n",
      "USER: For your previous answer, I love hawaiian!\n",
      "ASSISTANT: Hello again, Bank! It's nice to meet you. Hawaiian pizza is a great choice with pineapple and ham. Enjoy!\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "If I wanna find my new favorite pizza topping, what would you recommend?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n"
     ]
    }
   ],
   "source": [
    "print(lm.history[-1]['messages'][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99ba3742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': None,\n",
       " 'messages': [{'role': 'system',\n",
       "   'content': \"Your input fields are:\\n1. `chat_history` (str): Actual conversation history\\n2. `user_message` (str): Current user message\\nYour output fields are:\\n1. `response` (str): Assistant's response\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## chat_history ## ]]\\n{chat_history}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Respond based only on the provided chat_history. Examples are for format only.\"},\n",
       "  {'role': 'user',\n",
       "   'content': '[[ ## chat_history ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nHi, my name is Alice'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## response ## ]]\\nHello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"[[ ## chat_history ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## response ## ]]\\nYour name is Alice.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"[[ ## chat_history ## ]]\\nUSER: I like pizza\\nASSISTANT: That's great! Pizza is delicious. What's your favorite topping?\\n\\n[[ ## user_message ## ]]\\nI prefer pepperoni\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"[[ ## response ## ]]\\nPepperoni is a classic choice! It's one of the most popular pizza toppings.\\n\\n[[ ## completed ## ]]\\n\"},\n",
       "  {'role': 'user',\n",
       "   'content': \"[[ ## chat_history ## ]]\\nUSER: What's my favorite pizza topping?\\nASSISTANT: I'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\\nUSER: My name is Bank. Nice to meet you.\\nASSISTANT: Hello Bank! It's nice to meet you. If you ever want to share your favorite pizza topping, I'm here to listen.\\nUSER: What's my name?\\nASSISTANT: Hello again! Your name is Bank. It's nice to meet you.\\nUSER: For your previous answer, I love hawaiian!\\nASSISTANT: Hello again, Bank! It's nice to meet you. Hawaiian pizza is a great choice with pineapple and ham. Enjoy!\\n\\n[[ ## user_message ## ]]\\nIf I wanna find my new favorite pizza topping, what would you recommend?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
       " 'kwargs': {},\n",
       " 'response': ModelResponse(id='chatcmpl-a0da28e7-a019-4311-85fc-8f0f8a8fe3e8', created=1768741546, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## response ## ]]\\nHello again, Bank! For a new favorite pizza topping, you might enjoy trying something unique like truffle mushroom or spicy aioli with chicken and pineapple. These combinations can offer a delightful twist on traditional toppings. However, the best topping is ultimately what you enjoy the most!\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 591, 'completion_tokens': 68, 'total_tokens': 659}, cache_hit=False),\n",
       " 'outputs': ['[[ ## response ## ]]\\nHello again, Bank! For a new favorite pizza topping, you might enjoy trying something unique like truffle mushroom or spicy aioli with chicken and pineapple. These combinations can offer a delightful twist on traditional toppings. However, the best topping is ultimately what you enjoy the most!\\n\\n[[ ## completed ## ]]'],\n",
       " 'usage': {'prompt_tokens': 591, 'completion_tokens': 68, 'total_tokens': 659},\n",
       " 'cost': 0.0,\n",
       " 'timestamp': '2026-01-18T20:05:46.569143',\n",
       " 'uuid': 'fc2717e8-a746-4304-a929-f98d73beac58',\n",
       " 'model': 'bedrock-model',\n",
       " 'response_model': 'bedrock-model',\n",
       " 'model_type': 'chat'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1af8d699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What's my favorite pizza topping?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"I'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\"},\n",
       " {'role': 'user', 'content': 'My name is Bank. Nice to meet you.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Hello Bank! It's nice to meet you. If you ever want to share your favorite pizza topping, I'm here to listen.\"},\n",
       " {'role': 'user', 'content': \"What's my name?\"},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Hello again! Your name is Bank. It's nice to meet you.\"},\n",
       " {'role': 'user', 'content': 'For your previous answer, I love hawaiian!'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"Hello again, Bank! It's nice to meet you. Hawaiian pizza is a great choice with pineapple and ham. Enjoy!\"},\n",
       " {'role': 'user',\n",
       "  'content': 'If I wanna find my new favorite pizza topping, what would you recommend?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Hello again, Bank! For a new favorite pizza topping, you might enjoy trying something unique like truffle mushroom or spicy aioli with chicken and pineapple. These combinations can offer a delightful twist on traditional toppings. However, the best topping is ultimately what you enjoy the most!'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6989a6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': None,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': \"Your input fields are:\\n1. `chat_history` (str): Actual conversation history\\n2. `user_message` (str): Current user message\\nYour output fields are:\\n1. `response` (str): Assistant's response\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## chat_history ## ]]\\n{chat_history}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Respond based only on the provided chat_history. Examples are for format only.\"},\n",
       "   {'role': 'user',\n",
       "    'content': '[[ ## chat_history ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nHi, my name is Alice'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nHello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nYour name is Alice.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: I like pizza\\nASSISTANT: That's great! Pizza is delicious. What's your favorite topping?\\n\\n[[ ## user_message ## ]]\\nI prefer pepperoni\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"[[ ## response ## ]]\\nPepperoni is a classic choice! It's one of the most popular pizza toppings.\\n\\n[[ ## completed ## ]]\\n\"},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nWhat's my favorite pizza topping?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
       "  'kwargs': {},\n",
       "  'response': ModelResponse(id='chatcmpl-73358140-2a0c-464b-91e4-e5f55c4e20b3', created=1768741446, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"[[ ## response ## ]]\\nI'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\\n\\n[[ ## completed ## ]]\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 416, 'completion_tokens': 56, 'total_tokens': 472}, cache_hit=False),\n",
       "  'outputs': [\"[[ ## response ## ]]\\nI'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\\n\\n[[ ## completed ## ]]\"],\n",
       "  'usage': {'prompt_tokens': 416,\n",
       "   'completion_tokens': 56,\n",
       "   'total_tokens': 472},\n",
       "  'cost': 0.0,\n",
       "  'timestamp': '2026-01-18T20:04:06.398764',\n",
       "  'uuid': 'f06db348-4486-40bc-9db1-9c798f151a2a',\n",
       "  'model': 'bedrock-model',\n",
       "  'response_model': 'bedrock-model',\n",
       "  'model_type': 'chat'},\n",
       " {'prompt': None,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': \"Your input fields are:\\n1. `chat_history` (str): Actual conversation history\\n2. `user_message` (str): Current user message\\nYour output fields are:\\n1. `response` (str): Assistant's response\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## chat_history ## ]]\\n{chat_history}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Respond based only on the provided chat_history. Examples are for format only.\"},\n",
       "   {'role': 'user',\n",
       "    'content': '[[ ## chat_history ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nHi, my name is Alice'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nHello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nYour name is Alice.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: I like pizza\\nASSISTANT: That's great! Pizza is delicious. What's your favorite topping?\\n\\n[[ ## user_message ## ]]\\nI prefer pepperoni\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"[[ ## response ## ]]\\nPepperoni is a classic choice! It's one of the most popular pizza toppings.\\n\\n[[ ## completed ## ]]\\n\"},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: What's my favorite pizza topping?\\nASSISTANT: I'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\\n\\n[[ ## user_message ## ]]\\nMy name is Bank. Nice to meet you.\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
       "  'kwargs': {},\n",
       "  'response': ModelResponse(id='chatcmpl-5dc8c20f-a26a-41fc-8668-1ee88bbdff14', created=1768741456, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"[[ ## response ## ]]\\nHello Bank! It's nice to meet you. If you ever want to share your favorite pizza topping, I'm here to listen.\\n\\n[[ ## completed ## ]]\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 470, 'completion_tokens': 43, 'total_tokens': 513}, cache_hit=False),\n",
       "  'outputs': [\"[[ ## response ## ]]\\nHello Bank! It's nice to meet you. If you ever want to share your favorite pizza topping, I'm here to listen.\\n\\n[[ ## completed ## ]]\"],\n",
       "  'usage': {'prompt_tokens': 470,\n",
       "   'completion_tokens': 43,\n",
       "   'total_tokens': 513},\n",
       "  'cost': 0.0,\n",
       "  'timestamp': '2026-01-18T20:04:16.244379',\n",
       "  'uuid': 'b31b8235-73fd-448e-84dd-b15195687978',\n",
       "  'model': 'bedrock-model',\n",
       "  'response_model': 'bedrock-model',\n",
       "  'model_type': 'chat'},\n",
       " {'prompt': None,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': \"Your input fields are:\\n1. `chat_history` (str): Actual conversation history\\n2. `user_message` (str): Current user message\\nYour output fields are:\\n1. `response` (str): Assistant's response\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## chat_history ## ]]\\n{chat_history}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Respond based only on the provided chat_history. Examples are for format only.\"},\n",
       "   {'role': 'user',\n",
       "    'content': '[[ ## chat_history ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nHi, my name is Alice'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nHello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nYour name is Alice.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: I like pizza\\nASSISTANT: That's great! Pizza is delicious. What's your favorite topping?\\n\\n[[ ## user_message ## ]]\\nI prefer pepperoni\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"[[ ## response ## ]]\\nPepperoni is a classic choice! It's one of the most popular pizza toppings.\\n\\n[[ ## completed ## ]]\\n\"},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: What's my favorite pizza topping?\\nASSISTANT: I'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\\nUSER: My name is Bank. Nice to meet you.\\nASSISTANT: Hello Bank! It's nice to meet you. If you ever want to share your favorite pizza topping, I'm here to listen.\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
       "  'kwargs': {},\n",
       "  'response': ModelResponse(id='chatcmpl-0339d3da-145a-4511-b977-6dda17176357', created=1768741467, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"[[ ## response ## ]]\\nHello again! Your name is Bank. It's nice to meet you.\\n\\n[[ ## completed ## ]]\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 511, 'completion_tokens': 30, 'total_tokens': 541}, cache_hit=False),\n",
       "  'outputs': [\"[[ ## response ## ]]\\nHello again! Your name is Bank. It's nice to meet you.\\n\\n[[ ## completed ## ]]\"],\n",
       "  'usage': {'prompt_tokens': 511,\n",
       "   'completion_tokens': 30,\n",
       "   'total_tokens': 541},\n",
       "  'cost': 0.0,\n",
       "  'timestamp': '2026-01-18T20:04:27.058876',\n",
       "  'uuid': '66bed891-dcdb-4be4-85f9-6e46dc785268',\n",
       "  'model': 'bedrock-model',\n",
       "  'response_model': 'bedrock-model',\n",
       "  'model_type': 'chat'},\n",
       " {'prompt': None,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': \"Your input fields are:\\n1. `chat_history` (str): Actual conversation history\\n2. `user_message` (str): Current user message\\nYour output fields are:\\n1. `response` (str): Assistant's response\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## chat_history ## ]]\\n{chat_history}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Respond based only on the provided chat_history. Examples are for format only.\"},\n",
       "   {'role': 'user',\n",
       "    'content': '[[ ## chat_history ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nHi, my name is Alice'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nHello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nYour name is Alice.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: I like pizza\\nASSISTANT: That's great! Pizza is delicious. What's your favorite topping?\\n\\n[[ ## user_message ## ]]\\nI prefer pepperoni\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"[[ ## response ## ]]\\nPepperoni is a classic choice! It's one of the most popular pizza toppings.\\n\\n[[ ## completed ## ]]\\n\"},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: What's my favorite pizza topping?\\nASSISTANT: I'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\\nUSER: My name is Bank. Nice to meet you.\\nASSISTANT: Hello Bank! It's nice to meet you. If you ever want to share your favorite pizza topping, I'm here to listen.\\nUSER: What's my name?\\nASSISTANT: Hello again! Your name is Bank. It's nice to meet you.\\n\\n[[ ## user_message ## ]]\\nFor your previous answer, I love hawaiian!\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
       "  'kwargs': {},\n",
       "  'response': ModelResponse(id='chatcmpl-76f4e32b-37a7-4ae7-b097-a06f00ea2813', created=1768741497, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"[[ ## response ## ]]\\nHello again, Bank! It's nice to meet you. Hawaiian pizza is a great choice with pineapple and ham. Enjoy!\\n\\n[[ ## completed ## ]]\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 544, 'completion_tokens': 40, 'total_tokens': 584}, cache_hit=False),\n",
       "  'outputs': [\"[[ ## response ## ]]\\nHello again, Bank! It's nice to meet you. Hawaiian pizza is a great choice with pineapple and ham. Enjoy!\\n\\n[[ ## completed ## ]]\"],\n",
       "  'usage': {'prompt_tokens': 544,\n",
       "   'completion_tokens': 40,\n",
       "   'total_tokens': 584},\n",
       "  'cost': 0.0,\n",
       "  'timestamp': '2026-01-18T20:04:57.018786',\n",
       "  'uuid': '6bb66604-ad92-498b-b69a-17b7535357a7',\n",
       "  'model': 'bedrock-model',\n",
       "  'response_model': 'bedrock-model',\n",
       "  'model_type': 'chat'},\n",
       " {'prompt': None,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': \"Your input fields are:\\n1. `chat_history` (str): Actual conversation history\\n2. `user_message` (str): Current user message\\nYour output fields are:\\n1. `response` (str): Assistant's response\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## chat_history ## ]]\\n{chat_history}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Respond based only on the provided chat_history. Examples are for format only.\"},\n",
       "   {'role': 'user',\n",
       "    'content': '[[ ## chat_history ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nHi, my name is Alice'},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nHello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you. How can I help you today?\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': '[[ ## response ## ]]\\nYour name is Alice.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: I like pizza\\nASSISTANT: That's great! Pizza is delicious. What's your favorite topping?\\n\\n[[ ## user_message ## ]]\\nI prefer pepperoni\"},\n",
       "   {'role': 'assistant',\n",
       "    'content': \"[[ ## response ## ]]\\nPepperoni is a classic choice! It's one of the most popular pizza toppings.\\n\\n[[ ## completed ## ]]\\n\"},\n",
       "   {'role': 'user',\n",
       "    'content': \"[[ ## chat_history ## ]]\\nUSER: What's my favorite pizza topping?\\nASSISTANT: I'm sorry, but I don't have any information about your favorite pizza topping since we haven't discussed it yet. You can tell me if you'd like to share your favorite topping.\\nUSER: My name is Bank. Nice to meet you.\\nASSISTANT: Hello Bank! It's nice to meet you. If you ever want to share your favorite pizza topping, I'm here to listen.\\nUSER: What's my name?\\nASSISTANT: Hello again! Your name is Bank. It's nice to meet you.\\nUSER: For your previous answer, I love hawaiian!\\nASSISTANT: Hello again, Bank! It's nice to meet you. Hawaiian pizza is a great choice with pineapple and ham. Enjoy!\\n\\n[[ ## user_message ## ]]\\nIf I wanna find my new favorite pizza topping, what would you recommend?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
       "  'kwargs': {},\n",
       "  'response': ModelResponse(id='chatcmpl-a0da28e7-a019-4311-85fc-8f0f8a8fe3e8', created=1768741546, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## response ## ]]\\nHello again, Bank! For a new favorite pizza topping, you might enjoy trying something unique like truffle mushroom or spicy aioli with chicken and pineapple. These combinations can offer a delightful twist on traditional toppings. However, the best topping is ultimately what you enjoy the most!\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 591, 'completion_tokens': 68, 'total_tokens': 659}, cache_hit=False),\n",
       "  'outputs': ['[[ ## response ## ]]\\nHello again, Bank! For a new favorite pizza topping, you might enjoy trying something unique like truffle mushroom or spicy aioli with chicken and pineapple. These combinations can offer a delightful twist on traditional toppings. However, the best topping is ultimately what you enjoy the most!\\n\\n[[ ## completed ## ]]'],\n",
       "  'usage': {'prompt_tokens': 591,\n",
       "   'completion_tokens': 68,\n",
       "   'total_tokens': 659},\n",
       "  'cost': 0.0,\n",
       "  'timestamp': '2026-01-18T20:05:46.569143',\n",
       "  'uuid': 'fc2717e8-a746-4304-a929-f98d73beac58',\n",
       "  'model': 'bedrock-model',\n",
       "  'response_model': 'bedrock-model',\n",
       "  'model_type': 'chat'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd485101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your input fields are:\n",
      "1. `chat_history` (str): Previous chat history\n",
      "2. `user_message` (str): Current user message\n",
      "Your output fields are:\n",
      "1. `response` (str): Assistant's resposne\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## chat_history ## ]]\n",
      "{chat_history}\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "{user_message}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Response to user in a conversation\n",
      "\n",
      "[[ ## chat_history ## ]]\n",
      "User: Hi\n",
      "Assistant: Hello there!\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "How are you?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n"
     ]
    }
   ],
   "source": [
    "from dspy.adapters.chat_adapter import ChatAdapter\n",
    "import dspy\n",
    "\n",
    "# Define your signature\n",
    "class QA(dspy.Signature):\n",
    "    \"\"\"Answer questions accurately\"\"\"\n",
    "    question: str = dspy.InputField(desc=\"The question\")\n",
    "    answer: str = dspy.OutputField(desc=\"The answer\")\n",
    "\n",
    "# Create adapter and generate system prompt\n",
    "adapter = ChatAdapter()\n",
    "# signature = QA\n",
    "signature = ChatSignature\n",
    "\n",
    "# Generate system prompt components\n",
    "field_desc = adapter.format_field_description(signature)\n",
    "field_structure = adapter.format_field_structure(signature)\n",
    "task_desc = adapter.format_task_description(signature)\n",
    "\n",
    "# Combine into full system prompt\n",
    "system_prompt = f\"{field_desc}\\n{field_structure}\\n{task_desc}\"\n",
    "print(system_prompt)\n",
    "\n",
    "# Generate user message for specific inputs\n",
    "# inputs = {\"question\": \"What is the capital of Thailand?\"}\n",
    "inputs = {\"chat_history\": \"User: Hi\\nAssistant: Hello there!\", \"user_message\": \"How are you?\"}\n",
    "user_message = adapter.format_user_message_content(\n",
    "    signature=signature,\n",
    "    inputs=inputs,\n",
    "    main_request=True  # Adds output format reminder\n",
    ")\n",
    "print(\"\\n\" + user_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ccbad3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"Your input fields are:\\n1. `chat_history` (str): Previous chat history\\n2. `user_message` (str): Current user message\\nYour output fields are:\\n1. `response` (str): Assistant's resposne\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## chat_history ## ]]\\n{chat_history}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Response to user in a conversation\"},\n",
       " {'role': 'user',\n",
       "  'content': '[[ ## chat_history ## ]]\\nUser: Hi\\nAssistant: Hello there!\\n\\n[[ ## user_message ## ]]\\nHow are you?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37f95eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SYSTEM:\n",
      "Your input fields are:\n",
      "1. `question` (str): The question\n",
      "Your output fields are:\n",
      "1. `answer` (str): The answer\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## question ## ]]\n",
      "{question}\n",
      "\n",
      "[[ ## answer ## ]]\n",
      "{answer}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Answer questions accurately\n",
      "==================================================\n",
      "USER:\n",
      "[[ ## question ## ]]\n",
      "What is 2+2?\n",
      "==================================================\n",
      "ASSISTANT:\n",
      "[[ ## answer ## ]]\n",
      "4\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "==================================================\n",
      "USER:\n",
      "[[ ## question ## ]]\n",
      "What color is the sky?\n",
      "==================================================\n",
      "ASSISTANT:\n",
      "[[ ## answer ## ]]\n",
      "Blue\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "==================================================\n",
      "USER:\n",
      "[[ ## question ## ]]\n",
      "What is the capital of Thailand?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from dspy.adapters.chat_adapter import ChatAdapter\n",
    "import dspy\n",
    "\n",
    "# Define signature\n",
    "class QA(dspy.Signature):\n",
    "    \"\"\"Answer questions accurately\"\"\"\n",
    "    question: str = dspy.InputField(desc=\"The question\")\n",
    "    answer: str = dspy.OutputField(desc=\"The answer\")\n",
    "\n",
    "# Create demos\n",
    "demos = [\n",
    "    dspy.Example(question=\"What is 2+2?\", answer=\"4\").with_inputs(\"question\"),\n",
    "    dspy.Example(question=\"What color is the sky?\", answer=\"Blue\").with_inputs(\"question\"),\n",
    "]\n",
    "\n",
    "# Create adapter\n",
    "adapter = ChatAdapter()\n",
    "\n",
    "# Use the format() method to generate full messages including demos\n",
    "messages = adapter.format(\n",
    "    signature=QA,\n",
    "    demos=demos,\n",
    "    inputs={\"question\": \"What is the capital of Thailand?\"}\n",
    ")\n",
    "\n",
    "# Print all messages\n",
    "for msg in messages:\n",
    "    print(f\"{msg['role'].upper()}:\")\n",
    "    print(msg['content'])\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99e7312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Your input fields are:\\n1. `question` (str): The question\\nYour output fields are:\\n1. `answer` (str): The answer\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Answer questions accurately'},\n",
       " {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat is 2+2?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '[[ ## answer ## ]]\\n4\\n\\n[[ ## completed ## ]]\\n'},\n",
       " {'role': 'user', 'content': '[[ ## question ## ]]\\nWhat color is the sky?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '[[ ## answer ## ]]\\nBlue\\n\\n[[ ## completed ## ]]\\n'},\n",
       " {'role': 'user',\n",
       "  'content': '[[ ## question ## ]]\\nWhat is the capital of Thailand?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868694f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"Your input fields are:\\n1. `question` (str): user's questioin\\nYour output fields are:\\n1. `answer` (str): answer to the question\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Answer the question in concise.\"},\n",
       " {'role': 'user',\n",
       "  'content': \"[[ ## question ## ]]\\nwhat's love?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dspy.adapters.chat_adapter import ChatAdapter\n",
    "\n",
    "class QA(dspy.Signature):\n",
    "    \"\"\"Answer the question in concise.\"\"\"\n",
    "    question:str = dspy.InputField(desc=\"user's questioin\")\n",
    "    answer:str = dspy.OutputField(desc=\"answer to the question\")\n",
    "\n",
    "class QABot(dspy.Module):\n",
    "    def __init__(self):\n",
    "        self.program = dspy.Predict(QA)\n",
    "\n",
    "    def forward(self, question):\n",
    "        return self.program(question=question)\n",
    "\n",
    "lm = native_lm\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "chat_adapter = ChatAdapter()\n",
    "\n",
    "qa_bot = QABot()\n",
    "\n",
    "messages = chat_adapter.format(\n",
    "    signature=QA,\n",
    "    demos=[],\n",
    "    inputs={\"question\": \"what's love?\"}\n",
    ")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b5535d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"Your input fields are:\\n1. `question` (str): user's questioin\\nYour output fields are:\\n1. `answer` (str): answer to the question\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Answer the question in concise.\"},\n",
       " {'role': 'user',\n",
       "  'content': \"[[ ## question ## ]]\\nwhat's love?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_adapter.format(\n",
    "    signature=QA,\n",
    "    demos=[],\n",
    "    inputs={\"question\": \"what's love?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7ce76e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QA(question -> answer\n",
       "    instructions='Answer the question in concise.'\n",
       "    question = Field(annotation=str required=True json_schema_extra={'desc': \"user's questioin\", '__dspy_field_type': 'input', 'prefix': 'Question:'})\n",
       "    answer = Field(annotation=str required=True json_schema_extra={'desc': 'answer to the question', '__dspy_field_type': 'output', 'prefix': 'Answer:'})\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_bot.program.signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f946aa1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fun-with-dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
