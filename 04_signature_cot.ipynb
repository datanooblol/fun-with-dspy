{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe4b75b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04df16",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a881a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44aa5b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clear\n"
     ]
    }
   ],
   "source": [
    "# Clear DSPy's global disk cache\n",
    "if hasattr(dspy, 'cache') and hasattr(dspy.cache, 'disk_cache'):\n",
    "    dspy.cache.disk_cache.clear()\n",
    "    print(\"clear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "784f3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from typing import Any\n",
    "from package.base import DriverLM, ModelResponse, Usage\n",
    "\n",
    "\n",
    "def bedrock_request_fn(messages: list[dict[str, Any]], temperature: float = 0.0, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Request function for AWS Bedrock Converse API.\n",
    "    \n",
    "    DSPy passes messages in OpenAI format:\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"...\"},\n",
    "        {\"role\": \"user\", \"content\": \"...\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"...\"},\n",
    "        ...\n",
    "    ]\n",
    "    \n",
    "    Bedrock Converse API expects:\n",
    "    - system: list of system messages (separate parameter)\n",
    "    - messages: list of user/assistant turns (no system role)\n",
    "    \"\"\"\n",
    "    \n",
    "    client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "    \n",
    "    # Separate system messages from conversation\n",
    "    system_messages = []\n",
    "    conversation_messages = []\n",
    "    \n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] == \"system\":\n",
    "            system_messages.append({\"text\": msg[\"content\"]})\n",
    "        else:\n",
    "            conversation_messages.append({\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": [{\"text\": msg[\"content\"]}]\n",
    "            })\n",
    "    \n",
    "    # Build request\n",
    "    request_params = {\n",
    "        \"modelId\": \"us.amazon.nova-micro-v1:0\",\n",
    "        \"messages\": conversation_messages,\n",
    "        \"inferenceConfig\": {\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": kwargs.get(\"max_tokens\", 2048),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Add system messages if present\n",
    "    if system_messages:\n",
    "        request_params[\"system\"] = system_messages\n",
    "    \n",
    "    # Call Bedrock\n",
    "    response = client.converse(**request_params)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def bedrock_output_fn(response: dict) -> ModelResponse:\n",
    "    \"\"\"\n",
    "    Parse Bedrock Converse API response into ModelResponse.\n",
    "    \n",
    "    Bedrock response format:\n",
    "    {\n",
    "        \"output\": {\n",
    "            \"message\": {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"text\": \"...\"}]\n",
    "            }\n",
    "        },\n",
    "        \"usage\": {\n",
    "            \"inputTokens\": 100,\n",
    "            \"outputTokens\": 50,\n",
    "            \"totalTokens\": 150\n",
    "        },\n",
    "        \"stopReason\": \"end_turn\",\n",
    "        \"metrics\": {...}\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract content\n",
    "    content = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    \n",
    "    # Extract usage\n",
    "    usage_data = response.get(\"usage\", {})\n",
    "    usage = Usage(\n",
    "        prompt_tokens=usage_data.get(\"inputTokens\", 0),\n",
    "        completion_tokens=usage_data.get(\"outputTokens\", 0),\n",
    "        total_tokens=usage_data.get(\"totalTokens\", 0)\n",
    "    )\n",
    "    \n",
    "    # Get model ID from response metadata\n",
    "    model = response.get(\"ResponseMetadata\", {}).get(\"HTTPHeaders\", {}).get(\"x-amzn-bedrock-model-id\", \"bedrock-model\")\n",
    "    \n",
    "    return ModelResponse.from_text(text=content, usage=usage, model=model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "704d264e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'e4a8e47b-13bf-4ac6-b1af-5274112d2f4a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Sun, 18 Jan 2026 14:47:29 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '343',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'e4a8e47b-13bf-4ac6-b1af-5274112d2f4a'},\n",
       "  'RetryAttempts': 0},\n",
       " 'output': {'message': {'role': 'assistant',\n",
       "   'content': [{'text': \"Hello! How can I assist you today? Whether you have a question, need information, or just want to chat, I'm here to help. What's on your mind?\"}]}},\n",
       " 'stopReason': 'end_turn',\n",
       " 'usage': {'inputTokens': 1, 'outputTokens': 39, 'totalTokens': 40},\n",
       " 'metrics': {'latencyMs': 294}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_request_fn(messages=[{\"role\": \"user\", \"content\": \"hi\"}], temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8ba1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "native_lm = dspy.LM(\n",
    "    model=\"ollama/llama3.2-vision:11b\",\n",
    "    api_base=\"http://localhost:11434\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# Create Bedrock LM\n",
    "bedrock_lm = DriverLM(\n",
    "    request_fn=bedrock_request_fn,\n",
    "    output_fn=bedrock_output_fn,\n",
    "    temperature=0.0,\n",
    "    max_tokens=2048,\n",
    "    cache=True\n",
    ")\n",
    "\n",
    "bedrock_lm.clear_cache()\n",
    "\n",
    "# lm = native_lm\n",
    "lm = bedrock_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a4fca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatSignature(dspy.Signature):\n",
    "    \"\"\"Use the conversation_context to answer questions about the user.\"\"\"\n",
    "    conversation_context: str = dspy.InputField(desc=\"All previous messages in this conversation\")\n",
    "    user_message: str = dspy.InputField(desc=\"Current question\")\n",
    "    response: str = dspy.OutputField(desc=\"Answer using conversation_context\")\n",
    "\n",
    "\n",
    "demos = [\n",
    "    dspy.Example(\n",
    "        conversation_context=\"No previous messages\",\n",
    "        user_message=\"Hi, my name is Alice\",\n",
    "        response=\"Hello Alice! Nice to meet you.\"\n",
    "    ).with_inputs(\"conversation_context\", \"user_message\"),\n",
    "    \n",
    "    dspy.Example(\n",
    "        conversation_context=\"USER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you.\",\n",
    "        user_message=\"What's my name?\",\n",
    "        response=\"Your name is Alice.\"  # No greeting\n",
    "    ).with_inputs(\"conversation_context\", \"user_message\"),\n",
    "    \n",
    "    # Add this demo:\n",
    "    dspy.Example(\n",
    "        conversation_context=\"USER: Hi, my name is Alice\\nASSISTANT: Hello Alice!\\nUSER: What's my name?\\nASSISTANT: Your name is Alice.\",\n",
    "        user_message=\"How are you?\",\n",
    "        response=\"I'm doing well, thank you for asking!\"  # No greeting\n",
    "    ).with_inputs(\"conversation_context\", \"user_message\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e15b54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dspy.configure(lm=lm)\n",
    "qa_zero_shot = dspy.Predict(ChatSignature)\n",
    "qa_cot = dspy.ChainOfThought(ChatSignature)\n",
    "qa_zero_shot.demos = demos\n",
    "qa_cot.predict.demos = demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f929779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Predict(ChatSignature(conversation_context, user_message -> response\n",
       "     instructions='Use the conversation_context to answer questions about the user.'\n",
       "     conversation_context = Field(annotation=str required=True json_schema_extra={'desc': 'All previous messages in this conversation', '__dspy_field_type': 'input', 'prefix': 'Conversation Context:'})\n",
       "     user_message = Field(annotation=str required=True json_schema_extra={'desc': 'Current question', '__dspy_field_type': 'input', 'prefix': 'User Message:'})\n",
       "     response = Field(annotation=str required=True json_schema_extra={'desc': 'Answer using conversation_context', '__dspy_field_type': 'output', 'prefix': 'Response:'})\n",
       " ))]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_zero_shot.predictors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b72db46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Predict(StringSignature(conversation_context, user_message -> reasoning, response\n",
       "     instructions='Use the conversation_context to answer questions about the user.'\n",
       "     conversation_context = Field(annotation=str required=True json_schema_extra={'desc': 'All previous messages in this conversation', '__dspy_field_type': 'input', 'prefix': 'Conversation Context:'})\n",
       "     user_message = Field(annotation=str required=True json_schema_extra={'desc': 'Current question', '__dspy_field_type': 'input', 'prefix': 'User Message:'})\n",
       "     reasoning = Field(annotation=str required=True json_schema_extra={'prefix': \"Reasoning: Let's think step by step in order to\", 'desc': '${reasoning}', '__dspy_field_type': 'output'})\n",
       "     response = Field(annotation=str required=True json_schema_extra={'desc': 'Answer using conversation_context', '__dspy_field_type': 'output', 'prefix': 'Response:'})\n",
       " ))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_cot.predictors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ba9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_context = \"\"\"\n",
    "USER: My name is Bank. Nice to meet you.\n",
    "ASSISTANT: Hello Bank! It's nice to meet you.\n",
    "USER: What's my favorite pizza topping?\n",
    "ASSISTANT: I'm sorry, but I don't have that information. It's best to ask you directly about your favorite pizza topping.\n",
    "USER: For your previous answer, I love hawaiian!\n",
    "ASSISTANT: It's great to hear that you love Hawaiian pizza! It's a delicious choice.\n",
    "USER: If I wanna find my new favorite pizza topping, what would you recommend?\n",
    "ASSISTANT: That's a fun question! If you're looking to try something new, I'd recommend trying a pizza with truffle oil and mushrooms. The earthy flavor of the mushrooms combined with the richness of truffle oil can create a unique and delicious experience. However, the best topping is always personal preference, so feel free to experiment and find what you love!\n",
    "\"\"\"\n",
    "\n",
    "params = {\n",
    "    \"conversation_context\": conversation_context,\n",
    "    \"user_message\": \"What's my name?\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203294a9",
   "metadata": {},
   "source": [
    "# dspy.Predict: simple and lean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db55d9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    response='Your name is Bank.'\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_zero_shot(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c71c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2026-01-18T21:29:06.122353]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `conversation_context` (str): All previous messages in this conversation\n",
      "2. `user_message` (str): Current question\n",
      "Your output fields are:\n",
      "1. `response` (str): Answer using conversation_context\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "{conversation_context}\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "{user_message}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the conversation_context to answer questions about the user.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "No previous messages\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "Hi, my name is Alice\n",
      "\n",
      "\n",
      "\u001b[31mAssistant message:\u001b[0m\n",
      "\n",
      "[[ ## response ## ]]\n",
      "Hello Alice! Nice to meet you.\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "USER: Hi, my name is Alice\n",
      "ASSISTANT: Hello Alice! Nice to meet you.\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "What's my name?\n",
      "\n",
      "\n",
      "\u001b[31mAssistant message:\u001b[0m\n",
      "\n",
      "[[ ## response ## ]]\n",
      "Your name is Alice.\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "USER: Hi, my name is Alice\n",
      "ASSISTANT: Hello Alice!\n",
      "USER: What's my name?\n",
      "ASSISTANT: Your name is Alice.\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "How are you?\n",
      "\n",
      "\n",
      "\u001b[31mAssistant message:\u001b[0m\n",
      "\n",
      "[[ ## response ## ]]\n",
      "I'm doing well, thank you for asking!\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "\n",
      "USER: My name is Bank. Nice to meet you.\n",
      "ASSISTANT: Hello Bank! It's nice to meet you.\n",
      "USER: What's my favorite pizza topping?\n",
      "ASSISTANT: I'm sorry, but I don't have that information. It's best to ask you directly about your favorite pizza topping.\n",
      "USER: For your previous answer, I love hawaiian!\n",
      "ASSISTANT: It's great to hear that you love Hawaiian pizza! It's a delicious choice.\n",
      "USER: If I wanna find my new favorite pizza topping, what would you recommend?\n",
      "ASSISTANT: That's a fun question! If you're looking to try something new, I'd recommend trying a pizza with truffle oil and mushrooms. The earthy flavor of the mushrooms combined with the richness of truffle oil can create a unique and delicious experience. However, the best topping is always personal preference, so feel free to experiment and find what you love!\n",
      "\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "What's my name?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## response ## ]]\n",
      "Your name is Bank.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a93010d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': None,\n",
       " 'messages': [{'role': 'system',\n",
       "   'content': 'Your input fields are:\\n1. `conversation_context` (str): All previous messages in this conversation\\n2. `user_message` (str): Current question\\nYour output fields are:\\n1. `response` (str): Answer using conversation_context\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## conversation_context ## ]]\\n{conversation_context}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Use the conversation_context to answer questions about the user.'},\n",
       "  {'role': 'user',\n",
       "   'content': '[[ ## conversation_context ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nHi, my name is Alice'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## response ## ]]\\nHello Alice! Nice to meet you.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"[[ ## conversation_context ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you.\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## response ## ]]\\nYour name is Alice.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"[[ ## conversation_context ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice!\\nUSER: What's my name?\\nASSISTANT: Your name is Alice.\\n\\n[[ ## user_message ## ]]\\nHow are you?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"[[ ## response ## ]]\\nI'm doing well, thank you for asking!\\n\\n[[ ## completed ## ]]\\n\"},\n",
       "  {'role': 'user',\n",
       "   'content': \"[[ ## conversation_context ## ]]\\n\\nUSER: My name is Bank. Nice to meet you.\\nASSISTANT: Hello Bank! It's nice to meet you.\\nUSER: What's my favorite pizza topping?\\nASSISTANT: I'm sorry, but I don't have that information. It's best to ask you directly about your favorite pizza topping.\\nUSER: For your previous answer, I love hawaiian!\\nASSISTANT: It's great to hear that you love Hawaiian pizza! It's a delicious choice.\\nUSER: If I wanna find my new favorite pizza topping, what would you recommend?\\nASSISTANT: That's a fun question! If you're looking to try something new, I'd recommend trying a pizza with truffle oil and mushrooms. The earthy flavor of the mushrooms combined with the richness of truffle oil can create a unique and delicious experience. However, the best topping is always personal preference, so feel free to experiment and find what you love!\\n\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
       " 'kwargs': {},\n",
       " 'response': ModelResponse(id='chatcmpl-5d74961f-6a14-4e39-97ff-3af4a6b206a5', created=1768746546, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## response ## ]]\\nYour name is Bank.\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 593, 'completion_tokens': 19, 'total_tokens': 612}, cache_hit=False),\n",
       " 'outputs': ['[[ ## response ## ]]\\nYour name is Bank.\\n\\n[[ ## completed ## ]]'],\n",
       " 'usage': {'prompt_tokens': 593, 'completion_tokens': 19, 'total_tokens': 612},\n",
       " 'cost': 0.0,\n",
       " 'timestamp': '2026-01-18T21:29:06.122353',\n",
       " 'uuid': '50e23be4-c7ea-45d9-bb12-036dddf2e9e7',\n",
       " 'model': 'bedrock-model',\n",
       " 'response_model': 'bedrock-model',\n",
       " 'model_type': 'chat'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.history[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029385de",
   "metadata": {},
   "source": [
    "# dspy.ChainOfThought: adding reasoning as dspy.OutputField\n",
    "\n",
    "Note: based on inspect the prompt with demos, it seems dspy.CoT can handle normal demo with CoT as well by using  \n",
    "`[[ ## reasoning ## ]]\\nNot supplied for this particular example. \\n\\n`  \n",
    "\n",
    "Hence, dspy.Example can handle missing variables quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f1f6223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    reasoning='Based on the conversation context, the user has previously introduced themselves as \"Bank\". Therefore, the user\\'s name is Bank.',\n",
       "    response='Your name is Bank.'\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_cot(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba60f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[34m[2026-01-18T21:29:07.531635]\u001b[0m\n",
      "\n",
      "\u001b[31mSystem message:\u001b[0m\n",
      "\n",
      "Your input fields are:\n",
      "1. `conversation_context` (str): All previous messages in this conversation\n",
      "2. `user_message` (str): Current question\n",
      "Your output fields are:\n",
      "1. `reasoning` (str): \n",
      "2. `response` (str): Answer using conversation_context\n",
      "All interactions will be structured in the following way, with the appropriate values filled in.\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "{conversation_context}\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "{user_message}\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "{reasoning}\n",
      "\n",
      "[[ ## response ## ]]\n",
      "{response}\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "In adhering to this structure, your objective is: \n",
      "        Use the conversation_context to answer questions about the user.\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "No previous messages\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "Hi, my name is Alice\n",
      "\n",
      "\n",
      "\u001b[31mAssistant message:\u001b[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example. \n",
      "\n",
      "[[ ## response ## ]]\n",
      "Hello Alice! Nice to meet you.\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "USER: Hi, my name is Alice\n",
      "ASSISTANT: Hello Alice! Nice to meet you.\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "What's my name?\n",
      "\n",
      "\n",
      "\u001b[31mAssistant message:\u001b[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example. \n",
      "\n",
      "[[ ## response ## ]]\n",
      "Your name is Alice.\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "This is an example of the task, though some input or output fields are not supplied.\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "USER: Hi, my name is Alice\n",
      "ASSISTANT: Hello Alice!\n",
      "USER: What's my name?\n",
      "ASSISTANT: Your name is Alice.\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "How are you?\n",
      "\n",
      "\n",
      "\u001b[31mAssistant message:\u001b[0m\n",
      "\n",
      "[[ ## reasoning ## ]]\n",
      "Not supplied for this particular example. \n",
      "\n",
      "[[ ## response ## ]]\n",
      "I'm doing well, thank you for asking!\n",
      "\n",
      "[[ ## completed ## ]]\n",
      "\n",
      "\n",
      "\u001b[31mUser message:\u001b[0m\n",
      "\n",
      "[[ ## conversation_context ## ]]\n",
      "\n",
      "USER: My name is Bank. Nice to meet you.\n",
      "ASSISTANT: Hello Bank! It's nice to meet you.\n",
      "USER: What's my favorite pizza topping?\n",
      "ASSISTANT: I'm sorry, but I don't have that information. It's best to ask you directly about your favorite pizza topping.\n",
      "USER: For your previous answer, I love hawaiian!\n",
      "ASSISTANT: It's great to hear that you love Hawaiian pizza! It's a delicious choice.\n",
      "USER: If I wanna find my new favorite pizza topping, what would you recommend?\n",
      "ASSISTANT: That's a fun question! If you're looking to try something new, I'd recommend trying a pizza with truffle oil and mushrooms. The earthy flavor of the mushrooms combined with the richness of truffle oil can create a unique and delicious experience. However, the best topping is always personal preference, so feel free to experiment and find what you love!\n",
      "\n",
      "\n",
      "[[ ## user_message ## ]]\n",
      "What's my name?\n",
      "\n",
      "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
      "\n",
      "\n",
      "\u001b[31mResponse:\u001b[0m\n",
      "\n",
      "\u001b[32m[[ ## reasoning ## ]]\n",
      "Based on the conversation context, the user has previously introduced themselves as \"Bank\". Therefore, the user's name is Bank.\n",
      "\n",
      "[[ ## response ## ]]\n",
      "Your name is Bank.\n",
      "\n",
      "[[ ## completed ## ]]\u001b[0m\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dspy.inspect_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51260de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': None,\n",
       " 'messages': [{'role': 'system',\n",
       "   'content': 'Your input fields are:\\n1. `conversation_context` (str): All previous messages in this conversation\\n2. `user_message` (str): Current question\\nYour output fields are:\\n1. `reasoning` (str): \\n2. `response` (str): Answer using conversation_context\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## conversation_context ## ]]\\n{conversation_context}\\n\\n[[ ## user_message ## ]]\\n{user_message}\\n\\n[[ ## reasoning ## ]]\\n{reasoning}\\n\\n[[ ## response ## ]]\\n{response}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Use the conversation_context to answer questions about the user.'},\n",
       "  {'role': 'user',\n",
       "   'content': 'This is an example of the task, though some input or output fields are not supplied.\\n\\n[[ ## conversation_context ## ]]\\nNo previous messages\\n\\n[[ ## user_message ## ]]\\nHi, my name is Alice'},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## reasoning ## ]]\\nNot supplied for this particular example. \\n\\n[[ ## response ## ]]\\nHello Alice! Nice to meet you.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"This is an example of the task, though some input or output fields are not supplied.\\n\\n[[ ## conversation_context ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice! Nice to meet you.\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': '[[ ## reasoning ## ]]\\nNot supplied for this particular example. \\n\\n[[ ## response ## ]]\\nYour name is Alice.\\n\\n[[ ## completed ## ]]\\n'},\n",
       "  {'role': 'user',\n",
       "   'content': \"This is an example of the task, though some input or output fields are not supplied.\\n\\n[[ ## conversation_context ## ]]\\nUSER: Hi, my name is Alice\\nASSISTANT: Hello Alice!\\nUSER: What's my name?\\nASSISTANT: Your name is Alice.\\n\\n[[ ## user_message ## ]]\\nHow are you?\"},\n",
       "  {'role': 'assistant',\n",
       "   'content': \"[[ ## reasoning ## ]]\\nNot supplied for this particular example. \\n\\n[[ ## response ## ]]\\nI'm doing well, thank you for asking!\\n\\n[[ ## completed ## ]]\\n\"},\n",
       "  {'role': 'user',\n",
       "   'content': \"[[ ## conversation_context ## ]]\\n\\nUSER: My name is Bank. Nice to meet you.\\nASSISTANT: Hello Bank! It's nice to meet you.\\nUSER: What's my favorite pizza topping?\\nASSISTANT: I'm sorry, but I don't have that information. It's best to ask you directly about your favorite pizza topping.\\nUSER: For your previous answer, I love hawaiian!\\nASSISTANT: It's great to hear that you love Hawaiian pizza! It's a delicious choice.\\nUSER: If I wanna find my new favorite pizza topping, what would you recommend?\\nASSISTANT: That's a fun question! If you're looking to try something new, I'd recommend trying a pizza with truffle oil and mushrooms. The earthy flavor of the mushrooms combined with the richness of truffle oil can create a unique and delicious experience. However, the best topping is always personal preference, so feel free to experiment and find what you love!\\n\\n\\n[[ ## user_message ## ]]\\nWhat's my name?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\"}],\n",
       " 'kwargs': {},\n",
       " 'response': ModelResponse(id='chatcmpl-47a2a891-c2ec-4e7e-a088-416ea9aa0c26', created=1768746547, model='bedrock-model', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='[[ ## reasoning ## ]]\\nBased on the conversation context, the user has previously introduced themselves as \"Bank\". Therefore, the user\\'s name is Bank.\\n\\n[[ ## response ## ]]\\nYour name is Bank.\\n\\n[[ ## completed ## ]]', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 714, 'completion_tokens': 51, 'total_tokens': 765}, cache_hit=False),\n",
       " 'outputs': ['[[ ## reasoning ## ]]\\nBased on the conversation context, the user has previously introduced themselves as \"Bank\". Therefore, the user\\'s name is Bank.\\n\\n[[ ## response ## ]]\\nYour name is Bank.\\n\\n[[ ## completed ## ]]'],\n",
       " 'usage': {'prompt_tokens': 714, 'completion_tokens': 51, 'total_tokens': 765},\n",
       " 'cost': 0.0,\n",
       " 'timestamp': '2026-01-18T21:29:07.531635',\n",
       " 'uuid': 'f18afd2e-cdf5-47c5-b2f6-e0019b76a1e2',\n",
       " 'model': 'bedrock-model',\n",
       " 'response_model': 'bedrock-model',\n",
       " 'model_type': 'chat'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.history[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fun-with-dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
