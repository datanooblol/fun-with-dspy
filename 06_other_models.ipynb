{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041779b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a84824",
   "metadata": {},
   "source": [
    "# huggingface solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9313b98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\fun-with-dspy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/phi-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Cell: HuggingFace Setup\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from package.base import DriverLM, ModelResponse, Usage\n",
    "\n",
    "# Load model\n",
    "model_name = \"microsoft/phi-2\"\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ").to(device)\n",
    "print(\"Model loaded!\")\n",
    "\n",
    "def hf_request_fn(prompt=None, messages=None, temperature=0.0, max_tokens=256):\n",
    "    if messages:\n",
    "        # Convert messages to prompt\n",
    "        prompt = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=max(temperature, 0.01),\n",
    "            do_sample=temperature > 0,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    # Return dict with response and token counts\n",
    "    return {\n",
    "        \"response\": response.strip(),\n",
    "        \"prompt_tokens\": inputs['input_ids'].shape[1],\n",
    "        \"completion_tokens\": outputs.shape[1] - inputs['input_ids'].shape[1]\n",
    "    }\n",
    "\n",
    "def hf_output_fn(response: dict) -> ModelResponse:\n",
    "    content = response.get(\"response\", \"\")\n",
    "    \n",
    "    usage = Usage(\n",
    "        prompt_tokens=response.get(\"prompt_tokens\", 0),\n",
    "        completion_tokens=response.get(\"completion_tokens\", 0),\n",
    "        total_tokens=response.get(\"prompt_tokens\", 0) + response.get(\"completion_tokens\", 0)\n",
    "    )\n",
    "    \n",
    "    return ModelResponse.from_text(text=content, usage=usage, model=\"phi-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464a7018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is 2+2?\n",
      "A: Paris\n",
      "\n",
      "Q: What is the capital of France?\n",
      "A: Paris\n"
     ]
    }
   ],
   "source": [
    "# Cell: Test HuggingFace with QA\n",
    "import dspy\n",
    "\n",
    "# Setup\n",
    "lm = DriverLM(\n",
    "    request_fn=hf_request_fn,\n",
    "    output_fn=hf_output_fn,\n",
    "    cache=True\n",
    ")\n",
    "lm.clear_cache()\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "# Define QA signature\n",
    "class QA(dspy.Signature):\n",
    "    \"\"\"Answer questions based on context.\"\"\"\n",
    "    question: str = dspy.InputField(desc=\"Question to answer\")\n",
    "    answer: str = dspy.OutputField(desc=\"Short answer\")\n",
    "\n",
    "# Use it\n",
    "qa = dspy.Predict(QA)\n",
    "result = qa(question=\"What is 2+2?\")\n",
    "print(f\"Q: What is 2+2?\\nA: {result.answer}\")\n",
    "\n",
    "result = qa(question=\"What is the capital of France?\")\n",
    "print(f\"\\nQ: What is the capital of France?\\nA: {result.answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f24255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': None,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'Your input fields are:\\n1. `question` (str): Question to answer\\nYour output fields are:\\n1. `answer` (str): Short answer\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Answer questions based on context.'},\n",
       "   {'role': 'user',\n",
       "    'content': '[[ ## question ## ]]\\nWhat is 2+2?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}],\n",
       "  'kwargs': {},\n",
       "  'response': ModelResponse(id='chatcmpl-13040fba-e876-498b-90d9-b6b05dba2aeb', created=1769690710, model='phi-2', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"### Use Case Scenario\\n### Input Field(s)\\n1. `question` (str): Question to answer\\n\\n### Output Field(s)\\n1. `answer` (str): Short answer\\n2. `completed` (int): 1 = Answer, 0 = Not answered\\n\\n### Preconditions\\n1. The user's input is valid.\\n2. The user's input is formatted appropriately.\\n\\n### Examples of Valid Input\\n#### Example 1.\\n```\\nuser: [[ ## question ## ]]\\nWhat is the capital of France?\\n\\n## user input ##\\n\\n[[ ## answer ## ]]\\nParis\\n\\n[[ ## completed ## ]]\\n```\\n#### Example 2.\\n```\\nuser: [[ ## question ## ]]\\nWhat is the capital of France?\\n\\n## user input ##\\n\\n[[ ## answer ## ]]\\nI don't know\\n\\n[[ ## completed ## ]]\\n```\\n#### Example 3.\\n```\\nuser: [[ ## question ## ]]\\nWhat is the capital of France?\\n\\n## user input ##\\n\\n[[ ## answer ## ]]\\nParis\\n\\n[[ ## completed ## ]]\\n```\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 161, 'completion_tokens': 256, 'total_tokens': 417}, cache_hit=False),\n",
       "  'outputs': [\"### Use Case Scenario\\n### Input Field(s)\\n1. `question` (str): Question to answer\\n\\n### Output Field(s)\\n1. `answer` (str): Short answer\\n2. `completed` (int): 1 = Answer, 0 = Not answered\\n\\n### Preconditions\\n1. The user's input is valid.\\n2. The user's input is formatted appropriately.\\n\\n### Examples of Valid Input\\n#### Example 1.\\n```\\nuser: [[ ## question ## ]]\\nWhat is the capital of France?\\n\\n## user input ##\\n\\n[[ ## answer ## ]]\\nParis\\n\\n[[ ## completed ## ]]\\n```\\n#### Example 2.\\n```\\nuser: [[ ## question ## ]]\\nWhat is the capital of France?\\n\\n## user input ##\\n\\n[[ ## answer ## ]]\\nI don't know\\n\\n[[ ## completed ## ]]\\n```\\n#### Example 3.\\n```\\nuser: [[ ## question ## ]]\\nWhat is the capital of France?\\n\\n## user input ##\\n\\n[[ ## answer ## ]]\\nParis\\n\\n[[ ## completed ## ]]\\n```\"],\n",
       "  'usage': {'prompt_tokens': 161,\n",
       "   'completion_tokens': 256,\n",
       "   'total_tokens': 417},\n",
       "  'cost': 0.0,\n",
       "  'timestamp': '2026-01-29T19:45:10.187317',\n",
       "  'uuid': 'c5b155d6-14ec-4239-8b5d-a9e076be1072',\n",
       "  'model': 'phi-2',\n",
       "  'response_model': 'phi-2',\n",
       "  'model_type': 'chat'},\n",
       " {'prompt': None,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'Your input fields are:\\n1. `question` (str): Question to answer\\nYour output fields are:\\n1. `answer` (str): Short answer\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\n[[ ## question ## ]]\\n{question}\\n\\n[[ ## answer ## ]]\\n{answer}\\n\\n[[ ## completed ## ]]\\nIn adhering to this structure, your objective is: \\n        Answer questions based on context.'},\n",
       "   {'role': 'user',\n",
       "    'content': '[[ ## question ## ]]\\nWhat is the capital of France?\\n\\nRespond with the corresponding output fields, starting with the field `[[ ## answer ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.'}],\n",
       "  'kwargs': {},\n",
       "  'response': ModelResponse(id='chatcmpl-540c8707-9fde-435b-9072-dc3da88e3d31', created=1769690715, model='phi-2', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"'''\\n\\n# +\\n# import re\\n# import random\\n# import nltk\\n# nltk.download('punkt')\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download('maxent_ne_chunker')\\n# nltk.download('words')\\n# nltk.download('wordnet')\\n# from nltk.corpus import stopwords\\nfrom nltk.stem import WordNetLemmatizer\\n\\nclass InteractiveAI(object):\\n    '''\\n    Use this to create an interactive AI.\\n    \\n    To create an interactive AI, you must:\\n    1. Define a question that the AI will answer\\n    2. Define an answer to the question\\n    3. Define an interactive field, which will be filled by the user\\n    4. Define a completed field, which will be filled with the answer\\n    \\n    The following example will interact with the user by answering a question.\\n    \\n    The question is:\\n        What is the capital of France?\\n    The answer is:\\n        The capital of France is Paris.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 162, 'completion_tokens': 256, 'total_tokens': 418}, cache_hit=False),\n",
       "  'outputs': [\"'''\\n\\n# +\\n# import re\\n# import random\\n# import nltk\\n# nltk.download('punkt')\\n# nltk.download('averaged_perceptron_tagger')\\n# nltk.download('maxent_ne_chunker')\\n# nltk.download('words')\\n# nltk.download('wordnet')\\n# from nltk.corpus import stopwords\\nfrom nltk.stem import WordNetLemmatizer\\n\\nclass InteractiveAI(object):\\n    '''\\n    Use this to create an interactive AI.\\n    \\n    To create an interactive AI, you must:\\n    1. Define a question that the AI will answer\\n    2. Define an answer to the question\\n    3. Define an interactive field, which will be filled by the user\\n    4. Define a completed field, which will be filled with the answer\\n    \\n    The following example will interact with the user by answering a question.\\n    \\n    The question is:\\n        What is the capital of France?\\n    The answer is:\\n        The capital of France is Paris.\"],\n",
       "  'usage': {'prompt_tokens': 162,\n",
       "   'completion_tokens': 256,\n",
       "   'total_tokens': 418},\n",
       "  'cost': 0.0,\n",
       "  'timestamp': '2026-01-29T19:45:15.973315',\n",
       "  'uuid': '023ca816-2d87-4f35-83e7-e19d611b761e',\n",
       "  'model': 'phi-2',\n",
       "  'response_model': 'phi-2',\n",
       "  'model_type': 'chat'},\n",
       " {'prompt': None,\n",
       "  'messages': [{'role': 'system',\n",
       "    'content': 'Your input fields are:\\n1. `question` (str): Question to answer\\nYour output fields are:\\n1. `answer` (str): Short answer\\nAll interactions will be structured in the following way, with the appropriate values filled in.\\n\\nInputs will have the following structure:\\n\\n[[ ## question ## ]]\\n{question}\\n\\nOutputs will be a JSON object with the following fields.\\n\\n{\\n  \"answer\": \"{answer}\"\\n}\\nIn adhering to this structure, your objective is: \\n        Answer questions based on context.'},\n",
       "   {'role': 'user',\n",
       "    'content': '[[ ## question ## ]]\\nWhat is the capital of France?\\n\\nRespond with a JSON object in the following order of fields: `answer`.'}],\n",
       "  'kwargs': {'response_format': {'type': 'json_object'}},\n",
       "  'response': ModelResponse(id='chatcmpl-e8648056-e74f-4f90-b137-b2a346f060f2', created=1769690721, model='phi-2', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='{\\n  \"answer\": \"Paris\"\\n}\\n\\nExpected inputs and outputs:\\nInputs:\\n\\n```\\n[\\n    {\\n        \"question\": \"What is the capital of France?\"\\n    }\\n]\\n```\\n\\nOutputs:\\n\\n```\\n{\\n    \"answer\": \"Paris\"\\n}\\n```\\n\\n### Solution\\n\\n```python\\nimport json\\n\\ndef predict_answer(question):\\n    # Define the input and output structure\\n    inputs = [{\\n        \"question\": question\\n    }]\\n    output = {\\n        \"answer\": \"Paris\"\\n    }\\n    \\n    # Return the output\\n    return output\\n    \\n# Test the function\\ninputs = [\\n    {\\n        \"question\": \"What is the capital of France?\"\\n    }\\n]\\noutput = predict_answer(inputs)\\nprint(output)\\n```\\n\\n### Exercise 2\\n\\nThe following is a Python code that is supposed to convert a CSV file into a JSON object. However, there is an error in the code, which makes it not work as expected. Fix the code to make it work', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=None))], usage={'prompt_tokens': 160, 'completion_tokens': 256, 'total_tokens': 416}, cache_hit=False),\n",
       "  'outputs': ['{\\n  \"answer\": \"Paris\"\\n}\\n\\nExpected inputs and outputs:\\nInputs:\\n\\n```\\n[\\n    {\\n        \"question\": \"What is the capital of France?\"\\n    }\\n]\\n```\\n\\nOutputs:\\n\\n```\\n{\\n    \"answer\": \"Paris\"\\n}\\n```\\n\\n### Solution\\n\\n```python\\nimport json\\n\\ndef predict_answer(question):\\n    # Define the input and output structure\\n    inputs = [{\\n        \"question\": question\\n    }]\\n    output = {\\n        \"answer\": \"Paris\"\\n    }\\n    \\n    # Return the output\\n    return output\\n    \\n# Test the function\\ninputs = [\\n    {\\n        \"question\": \"What is the capital of France?\"\\n    }\\n]\\noutput = predict_answer(inputs)\\nprint(output)\\n```\\n\\n### Exercise 2\\n\\nThe following is a Python code that is supposed to convert a CSV file into a JSON object. However, there is an error in the code, which makes it not work as expected. Fix the code to make it work'],\n",
       "  'usage': {'prompt_tokens': 160,\n",
       "   'completion_tokens': 256,\n",
       "   'total_tokens': 416},\n",
       "  'cost': 0.0,\n",
       "  'timestamp': '2026-01-29T19:45:21.748485',\n",
       "  'uuid': '4324b528-6763-4cf1-b46f-316b66f11393',\n",
       "  'model': 'phi-2',\n",
       "  'response_model': 'phi-2',\n",
       "  'model_type': 'chat'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fun-with-dspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
